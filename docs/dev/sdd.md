# FloPy 4 software design description (SDD)

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [Principles](#principles)
- [Overview](#overview)
  - [Runtime](#runtime)
  - [Plugins](#plugins)
  - [Objects](#objects)
    - [Counting the ways](#counting-the-ways)
    - [Context resolution](#context-resolution)
    - [`Dict` mimicry](#dict-mimicry)
    - [Impedance mismatch](#impedance-mismatch)
  - [Parameters](#parameters)
    - [Arrays](#arrays)
    - [Tables](#tables)
    - [Laziness](#laziness)
    - [Signals](#signals)
    - [Units](#units)
  - [Code generation](#code-generation)
  - [IO](#io)
    - [Overview](#overview-1)
    - [Unified access](#unified-access)
    - [Converters](#converters)
    - [Codecs](#codecs)
    - [Parsers](#parsers)
- [Resources](#resources)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

This document drafts a design philosophy and architecture
for FloPy 4. This is a tentative document. A proof of
concept is underway.

## Principles

1. Separation of concerns

We want to avoid coupling FloPy's core framework to the MODFLOW 6 framework and
to the MODFLOW 6 input format. We want to separate abstract requirements, which
all hydrologic simulations might share, from details of the program's structure
and its expected input format. We want the same core framework to drive MODFLOW
6, MODFLOW 2005, LGR, NWT, GRIDGEN, etc &mdash; ideally any hydrologic simulator.

We also want to avoid polluting the core framework with post-processing details
like plotting, aggregations, input/output, etc.

IO, for instance, is at the boundary of an application and should only affect
the object model in rare instances.

*any more?*

## Overview

We propose a generic framework for hydrologic models.

FloPy can consist of plugins, each defining a wrapper
for a given hydrologic program. Programs are expected
to provide an unambiguous input specification.

FloPy can provide a basic set of building blocks with
which a program's input parameters can be defined and
configured. This will consist of parameters and their
contexts. The former are leaves, the latter nodes in
an input context tree (this is made rigorous below).

Provided an input specification for a program, FloPy
can generate an object-oriented Python interface for
it. This will consist of an **object model** (input
data model) and **IO machinery** (data access layer)
at minimum. (These are separate concerns and will be
coupled only where necessary/appropriate.)

Once these exist, they *are* the specification &mdash;
specification documents should be derivable in reverse.

FloPy will provide a **plugin runtime** which accepts
a program selection and an input configuration.

The runtime will validate the latter, run the program,
report its progress, and make its results available.

### Runtime

FloPy will provide a plugin runtime whose purpose is to
wrap and run arbitrary hydrologic programs. We consider
the **simulation** the fundamental abstraction here; a
simulation is a *plan for how to execute a program*. A
simulation is *not* the execution of the program itself
*nor* the harness which drives the program:

This at odds with the standard terminology in MODFLOW 6,
where a simulation *is precisely* the runtime. FloPy is
an interface to modeling codes, and as such, adopts the
view that one might as well call the thing that becomes
the simulation the simulation; this seems like a benign
and (maybe even appropriate) effacement of a meaningful
distinction for reasons of precedent and familiarity.

A distinct abstraction can represent the "task" running
the program, and a third can represent its output. `Run`
for the former and `Result` for the latter maybe?

All runs can have an autogenerated GUID and a name, with
the name defaulting to the GUID if the run is anonymous.

Scheduling seems like it could benefit from asynchrony.
While programs should ideally make maximal use of the
resources provided to them, one might want to run more
than one single-threaded program at once, without the
need for a separate Python interpreter for each one.

An awaitable (coroutine-based) API, returning futures
instead of blocking, could allow  an arbitrary number
of concurrent runs.

We can provide a traditional synchronous alternative
which runs the simulation directly.

### Plugins

A developer will first implement a plugin supporting some
hydrologic modeling program. This *must* include an input
specification and *may* involve overriding hooks provided
by the FloPy framework to customize various behavior. If
the program requires input in a bespoke language, or its
specification is written in such a language, the plugin
*must* include an explicit language specification, which
is used to generate a parser for the language. By default
FloPy will support standard data interchange formats such
such as JSON, TOML, and YAML.

Provided a valid input specification, FloPy will generate
a Python interface for the given program. At minimum this
consists of an object model and a data access layer. The
former is user-facing, and will invoke the latter behind
the scenes to read and write plugin input configurations.

A model developer will interact with the plugin, using the
interface layer to construct a model, and with the runtime
framework, to run the model. The interface layer will also
provide access to model results.

### Objects

We want:

- an intuitive, consistent, & expressive interface
to a broad range of programs.

- a small core codebase and a largely autogenerated 
user-facing input data model.

- an unsurprising and uncomplicated core framework
accessible to new contributors.

- more consistent (and fewer) points of entry.

- easy access to a program's input specification.

- easy access to a simulation's configuration.

- hierarchical namespacing, address resolution, and
value lookups.

- context-aware parameters and automatic enforcement
of program invariants.

...and more.

#### Counting the ways

Python has several approaches to defining data classes:
dictionaries, named tuples, plain classes, `dataclasses`.
The latter aim to make it easier to give a class a nice
`__repr__`, structural equality, and other conveniences.

`dataclasses` is derived from an older project called
[`attrs`](https://www.attrs.org/en/stable/) which has
some extra powers, including hooks for validation and
transformation, introspection tools, and more. Its age
does not appear to be a problem; the developer remains
active and it's on a regular release cadence with many
active users.

Since `attrs` solves several of our problems at once,
we aim to build a prototype of the core object model
on it.

Peripheral concerns (e.g. plotting/exporting) can be
handled by mixins, so we can avoid polluting the core
classes and also avoid the diamond problem.

#### Context resolution

Each component must be able to resolve its location in
context. This will support hierarchical addressing, as
is used for the MF6 memory manager. It may also inform
certain user-facing operations (for instance, a method
may work differently if a component is independent vs
an element in a simulation).

It is also simply convenient to ask a component: what
are you attached to? The component should be able to
display a tree showing its own position in context.

Parent pointers might be implemented as weak references
to avoid memory leaks; e.g., if a component is removed
from a simulation and the simulation is descarded, then
we want the garbage collector to be free to collect it,
and we want a finalizer callback to set the component's
`parent` to None.

This should also help provide nice string representations.

#### `Dict` mimicry

The dictionary is a ubiquitous data container, useful
for e.g. passing keyword arguments, and for potential
interop with 3rd-party libraries accepting dict-like
objects. It would be nice if object model components
could act like (or be easily convertible to) dicts.

#### Impedance mismatch

It is not always easy to convert a data source to/from
hierarchical data structures. In relational databases
this is called "object-relational impedance mismatch".

FloPy faces a similar challenge: we cannot assume that
programs will (and MF6 does not) specify input format
in, or read input from, a hierarchical configuration
language/file.

The MF6 DFN specification, for instance, consists of a
file for each component, each of which is a flat list
of parameters; an implicit parameter type hierarchy,
along with an attribute `in_record`, determine where
the parameter is situated in the input context (i.e.,
if it is part of record, or within a list). The block
membership structure is defined by `block` attributes.
This is akin to a single database table (imagining an
extra column `file_name` or similar, which identifies
the DFN file and the component it specifies).

From this, FloPy must generate a nested object model.
This means distinguishing scalars from composites. It
means plugin developers need to think deeply about how
they map a linguistically picky program's input to the 
FloPy data model. And it means we, as developers, need
to think deeply about the FloPy data model.

### Parameters

A parameter is a program input variable.

Parameters are the core of the FloPy4 data model.

Parameters are primitives or composites of such.

The data model should be agnostic to any program
supported by FloPy4; plugins should hide details
of the program's data representation and present
the same core object model and parameter types.

As described above in the object model section: a
simulation is the root context. A context contains
parameters. A parameter can be a scalar or another
context. A scalar is a leaf in the context tree.

Scalars are Python primitives. These are: int,
float, boolean, string, path, array, or table.

Ideally a data model would be dependency-agnostic,
but we view NumPy and Pandas as de facto standard
library and accept them as array/table primitives.

If we ever need to provide array/table abstractions
of our own, we could take inspiration from
[astropy](https://github.com/astropy/astropy).

A record is a context whose parameters are all
scalars; no nested contexts. We will consider
this a `Dict` for practical use though it will
need implementing as an `attrs`-based class so
its parameter spec is discoverable upon import.

A list can contain a single parameter type or a
union of parameter types.

On this view, an MF6 keystring is a `typing.Union`
of multiple records. The period block is a list of
unions of records.

We envision a nested hierarchy of `attrs`-based
classes, all acting like dictionaries, making up
the context tree. Each of these has parameters
and/or other classes as members.

FloPy can thus define a parameter as:

```python
from typing import Dict, List
from numpy.typing import ArrayLike
from pandas import DataFrame

Scalar = bool | int | float | str | Path
Record = Dict[str, Scalar]
List = List[Scalar | Record]
Array = ArrayLike
Table = DataFrame
Param = Scalar | Record | List | Array | Table
```

This is proposed as a general foundation onto which
it should be possible to map input specifications
for a broad range of programs, not only MODFLOW 6.

#### Arrays

We can accept any `numpy.typing.ArrayLike` value,
whether a standard `ndarray` or some other flavor,
so long as it acts like an array.

We can implement `ArrayLike`s of our own if there is
a good case for it, e.g. constant or layered arrays:

- `ConstantArray`: store constant array as a scalar
- `LayeredArray`: convenient access to array layers

These can be factored out of the original `MFArray`
implementation.

#### Tables

We can accept any `pd.DataFrame` (or subclass) as
tabular input.

We draw a sharp distinction between *regularly*
shaped (tabular) MF6 list input and unstructured
list input. We require these be distinguished by
type hints. The former can use `pandas` builtin
IO routines; the latter needs custom treatment.

We can store parameter specification information
in the `DataFrame.attrs` property or by way of
[custom accessors](https://pandas.pydata.org/pandas-docs/stable/development/extending.html#registering-custom-accessors).

#### Laziness

We recognize a distinction between two types of
parameter: configuration and data. This isn't
always valid, but it can be useful: for example
we might consider scalars and standalone records
configuration parameters, and load them eagerly
by default, then load array, lists, and tables
only on first access, unless the user requests
eager loading.

#### Signals

Parameter constraints and other dependencies can
be implemented with the observer pattern.

At import time, a program's input model can look
at its parameters, determine their dependencies,
and subscribe dependents to their sources. When
a source parameter changes at runtime, its value
is broadcast to all listeners, which can run a
callback to modify their own values in response.

`attrs` provides hooks which may make this kind
of class customization fairly straightforward.

#### Units

It may be worth considering support for units via e.g.
[pint](https://github.com/hgrecco/pint). Though units
are not required for MODFLOW simulations, they can be
passed through for purposes ofdocumentation, and some
other programs may require them.

### Code generation

Generated files are currently version controlled
in FloPy 3.

Code generation is a developer task, which we do
immediately before each release, or when needed
for development/modification of MF6 components.

This has the benefit that Python source files for
the MF6 input model can be inspected in an editor.
The discipline required to update them in version
control is slightly burdensome.

We could consider moving this task to install time,
and removing Python source files from versioning.
This would preserve the ability to inspect them,
while removing the question about when to update
them.

We could ship the latest IO spec with each FloPy
release. Upon installation, FloPy would generate
components from this specification. Users could
use the existing developer tools to regenerate
them for a different specification if needed,
as is currently possible. Users could still
inspect the classes in an IDE, too.

We could consider splitting out a new command
just to retrieve an input specification: e.g.
`get_spec`. The existing `generate_classes`
command could simply generate source code.
We could combine these into a new command
`migrate` which is equivalent to the current
behavior of `generate_classes`.

A final option is to generate an object model
entirely dynamically, without any source files
This may present debugging challenges. If we go
this route, an informative `repr()` becomes even
more critical, as it will be the only way to get
the specification from an object model class
(besides of the course the documentation).

### IO

#### Overview

From the [principles](#principles):

> IO... is at the boundary of an application and should only affect
the object model in rare instances.

Access to IO can be unified with a decorator or mixin applied to object model classes,
which accesses a global registry of supported output formats and de/encoding machinery.
Defaults can be configured at this level.

Format can also be configurable on a per-simulation basis, or at read/write time
via method parameters.

IO can thus be separated into several layers:

- An `io` class decorator (or mixin) which provides `load` and `write` methods
  to the object model
- `cattrs` converters to map the object model to/from scalar/container
  primitives (un/structuring)
- Encoders/decoders for any number of serialization formats, which translate
  primitives to strings
- Language specification and parser for custom encodings
  can be achieved with `lark`

<br/>

```mermaid
sequenceDiagram
  box object model (attrs)
  participant Component
  end
  box io (decorator, mixin)
  participant IO
  end
  box converters (cattrs)
  participant Converter
  end
  box codecs (cattrs, custom)
  participant JSON
  participant TOML
  participant YAML
  participant NetCDF
  participant MF6
  end
  box custom parsers (lark)
  participant MF6Parser
  end
  box specification (DFNs, etc)
  participant DFN
  end
  Component-->IO: provides load()/write()
  IO->>Converter: structure()/unstructure()
  Converter->>TOML: load(s)/dump(s)
  Converter->>YAML: load(s)/dump(s)
  Converter->>JSON: load(s)/dump(s)
  Converter->>NetCDF: load(s)/dump(s)
  Converter->>MF6: load(s)/dump(s)
  MF6->>MF6Parser: parse()
  MF6Parser-->DFN: defines grammar
```

#### Unified access

A small set of class decorators could provide unified access to
IO for object model classes. Alternatively these could be mixins.

The decorator (or mixin) could manage access to a global registry
of converters, and make them available to object model `load()` /
`write()` methods taking a `format="..."` or similarly named argument.

To cleanly support external data files, we could consider noting 
them (maybe with a context manager) when `write()` is called on
context in which they appear, and only coming back to them once
all components in the hierarchy have written their own files.

Model output objects could consider aliasing `write()` to `export()`
for consistency with the current FloPy convention.

#### Converters

A unified IO layer can interact with lower-level IO routines via
a **conversion** layer. Conversion is structuring (instantiation
from Python primitives or containers) and unstructuring (mapping
back to Python builtins) of the object model from and to generic
data structures.

The core of this layer is a `cattrs.Converter`, each of which is
associated with some configuration language. We could define a
custom converter for the MF6 input format, wrapping a custom
codec (encoder/decoder; see below).

We want to be able to check invariants on context initialization;
i.e., input model validation. We may also want to transform the
value of any particular parameter based on the values of other
parameters. Conversion time is a suitable contest for validation
and transformation.

A `validator` or `converter` may be registered with `attrs`-based
class fields. We could define custom validators or converters to
e.g. constrain the shape of an array, or to try to slice it upon
a change in grid dimensions.

Alternatively we could consider `pydantic` for this functionality.

#### Codecs

A codec encodes Python primitives and containers to strings and
decodes strings into Python primitives and containers. The `json`
module in the standard library is a prominent example. We could
provide a custom codec for the MF6 input format. This requires
a custom parser, which (we have discovered) is cumbersome to
implement by hand.

#### Parsers

In the initial proof of concept we had each component in
the object model parsing itself from text. There is much
wrong with this as discussed above, but another issue is
that doing it by hand is hard and error-prone.

An alternative to rolling our own backtracking recursive
descent parser is to define the MF6IO specification, and
let a [parser generator](https://en.wikipedia.org/wiki/Comparison_of_parser_generators)
make one for us. This is likely to perform better than a
custom implementation, and it forces us to make the input
specification precise.

We could define an ABNF or EBNF specification for the MF6
input format. This is done by other languages, e.g. TOML
https://github.com/toml-lang/toml/blob/1.0.0/toml.abnf.
A parser generator could consume this. One option might be 
[lark](https://lark-parser.readthedocs.io/en/latest/index.html).

## Resources

- https://docs.python.org/3/reference/datamodel.html?emulating-container-types=#emulating-generic-types
- https://docs.python.org/3/library/asyncio-task.html
- https://docs.python.org/3/library/weakref.html
- https://en.wikipedia.org/wiki/Observer_pattern
- https://www.attrs.org/en/stable/index.html#
- https://catt.rs/en/stable/index.html#
- https://docs.pydantic.dev/latest/
- https://threeofwands.com/why-i-use-attrs-instead-of-pydantic/
- https://code.djangoproject.com/wiki/DevModelCreation
- https://github.com/astropy/astropy: astronomy project, nicely designed core data structures and io layer
- https://github.com/pyinat/pyinaturalist/tree/main/pyinaturalist/models: attrs for a web API client
- https://numpy.org/doc/stable/user/basics.interoperability.html: numpy interop for custom array-likes
- https://github.com/lark-parser/lark
- https://en.wikipedia.org/wiki/Hierarchical_database_model
- https://en.wikipedia.org/wiki/Object_model
- https://en.wikipedia.org/wiki/Object%E2%80%93relational_impedance_mismatch
